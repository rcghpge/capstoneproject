# Capstone Technical Documentation (Core Blueprint Draft)

## Overview

This document provides a preliminary structured blueprint and framework for integrating **SQL/SQLite**, **Python ML pipelines**, and **Tableau dashboards** into a cohesive, 
reproducible body of work. The goal is to demonstrate an end-to-end workflow from raw data ingestion to final model deployment and visualization, ensuring reproducibility, stability, and 
interpretability in the high-dimensional setting (p ≫ n). It serves as a baseline for formulating this body of work.

---

## 1. Data Ingestion & Management (SQL/SQLite)

**Role:** Upstream data backbone.

* **Storage:** Maintain raw and intermediate datasets in SQLite for lightweight, portable data warehousing.
* **Preprocessing:** Execute initial filtering, joins, and aggregations at the SQL layer to enforce data consistency and reduce redundancy.
* **Integrity checks:** Validate completeness, enforce constraints, and audit data lineage.
* **Documentation:** SQL queries act as reproducible recipes, ensuring consistent preprocessing logic inside and outside the ML pipeline.

**Fit:** SQL/SQLite sits **before feature engineering**, delivering structured, clean data into the Python environment.

---

## 2. Feature Engineering & Modeling (Python ML Stack)

**Role:** Core ML engine.

### Methodologies

* **Imputation:** MICE, MissForest, miceforest, SoftImpute, KNN Imputer.
* **Feature Selection & Reduction:** Elastic Net, Boruta, SparsePCA, PLS/PLS-DA, Stability Selection, mRMR, SIS.
* **Regularization Models:** Ridge, Lasso (L1), Elastic Net.
* **Protocols:** Nested CV, bootstrap stability analysis, permutation testing, leakage prevention.

### Workflow

1. Impute missing values per CV fold.
2. Apply filter + embedded feature selection.
3. Reduce dimensionality where necessary (SparsePCA/PLS).
4. Train candidate models (Ridge, EN, GBDT, SVM).
5. Evaluate under nested CV.
6. Finalize and serialize models (e.g., `Pipeline.pkl`, ONNX, PMML).

**Fit:** Python executes the **full feature-engineering and modeling process**, serving as the centerpiece of the pipeline.

---

## 3. Analytics & Visualization (Tableau)

**Role:** Downstream communication and decision layer.

* **Integration:** Connect Tableau directly to SQLite databases or Python outputs (`.csv`, `.parquet`, `.db`).
* **Exploratory Dashboards:** Monitor missingness patterns, feature-target relations, and initial EDA summaries.
* **Final Dashboards:** Present model insights for stakeholders.

  * Prediction probabilities (e.g., churn likelihood, infant mortality risk).
  * Feature importance rankings.
  * Stability selection frequencies.
  * Residual diagnostics.
  * Subgroup / stratified analysis.

**Fit:** Tableau is the **delivery mechanism**, translating technical results into accessible insights.

---

## 4. Robust Framework Options (Final Models)

For reliable finalization, the following traditional ML frameworks are recommended:

* **Linear shrinkage models:** Ridge, Elastic Net, Lasso.
* **Tree ensembles:** Gradient Boosted Trees (XGBoost, LightGBM, CatBoost) with permutation importance and early stopping.
* **Kernelized models:** SVM (linear or low-degree polynomial kernels).
* **Calibrated probabilities:** Platt scaling or isotonic regression.
* **Model averaging/ensembling:** Simple stacked ensemble (e.g., Elastic Net + GBDT).

**Finalize step:**

1. Train under nested CV.
2. Select top 2–3 candidate models.
3. Apply calibration if needed.
4. Lock full preprocessing + modeling pipeline.
5. Save artifacts for reproducibility.
6. Integrate predictions into Tableau dashboards.

---

## 5. End-to-End Flow

**SQL/SQLite → Python ML Framework → Final Model(s) → Tableau Dashboards**

* **SQL/SQLite**: Structured storage, preprocessing, and integrity enforcement.
* **Python ML**: Robust feature engineering, modeling, and evaluation.
* **Tableau**: Visualization, communication, and stakeholder-facing insights.

---

## ✅ Key Takeaway

This pipeline ensures the capstone project is not only an academic exercise but a **production-grade, reproducible workflow** that spans data warehousing, machine learning, and business intelligence visualization.

---
